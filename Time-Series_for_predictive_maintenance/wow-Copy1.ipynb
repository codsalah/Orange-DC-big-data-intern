{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67106548-1b4c-422b-a812-8f5a041e98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c217f58d-f6f4-40d5-a2be-2375ec4124ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IoT Data Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab78394-6be8-4c27-8b22-a8b6903c7c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|Date(UTC)|UnixTimeStamp|      Value|\n",
      "+---------+-------------+-----------+\n",
      "|7/30/2015|   1438214400|39316.09375|\n",
      "|7/31/2015|   1438300800|36191.71875|\n",
      "| 8/1/2015|   1438387200| 27705.9375|\n",
      "| 8/2/2015|   1438473600| 28223.4375|\n",
      "| 8/3/2015|   1438560000|27976.71875|\n",
      "| 8/4/2015|   1438646400| 28478.4375|\n",
      "| 8/5/2015|   1438732800|27528.59375|\n",
      "| 8/6/2015|   1438819200|27075.46875|\n",
      "| 8/7/2015|   1438905600|27437.65625|\n",
      "| 8/8/2015|   1438992000| 27943.4375|\n",
      "| 8/9/2015|   1439078400|27178.28125|\n",
      "|8/10/2015|   1439164800|27817.34375|\n",
      "|8/11/2015|   1439251200| 28027.8125|\n",
      "|8/12/2015|   1439337600| 27370.9375|\n",
      "|8/13/2015|   1439424000|  28268.125|\n",
      "|8/14/2015|   1439510400|31106.71875|\n",
      "|8/15/2015|   1439596800|28512.65625|\n",
      "|8/17/2015|   1439769600|28118.90625|\n",
      "|8/16/2015|   1439683200|27094.53125|\n",
      "|8/18/2015|   1439856000| 26162.8125|\n",
      "+---------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data please\n",
    "file_path = \"data/sensor_data.csv\"  \n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b40a034-b49e-4d27-9144-99c159cb1aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date(UTC): string (nullable = true)\n",
      " |-- UnixTimeStamp: integer (nullable = true)\n",
      " |-- Value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96bb08e4-51ab-4be3-b002-fedef4a0b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+------------------+\n",
      "|summary|Date(UTC)|       UnixTimeStamp|             Value|\n",
      "+-------+---------+--------------------+------------------+\n",
      "|  count|     1164|                1164|              1164|\n",
      "|   mean|     null|          1.488456E9|26100.344233247422|\n",
      "| stddev|     null|2.9044410904681817E7| 5148.035180248755|\n",
      "|    min| 1/1/2016|          1438214400|           14852.5|\n",
      "|    max| 9/9/2018|          1538697600|       39316.09375|\n",
      "+-------+---------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c69dce-ea3d-44c8-9453-9e95d3ec5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the JDBC driver path\n",
    "jdbc_driver_path = \"/home/jovyan/data/postgresql-42.3.6.jar\"\n",
    "\n",
    "# Create a Spark session and include the JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL Connection\") \\\n",
    "    .config(\"spark.jars\", jdbc_driver_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# PostgreSQL connection properties\n",
    "db_url = \"jdbc:postgresql://localhost:5432/project3db\"  \n",
    "db_properties = {\n",
    "    \"user\": \"user\",  \n",
    "    \"password\": \"userpassword\",  \n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "postgres_df = spark.read.jdbc(url=db_url, table=\"your_table_name\", properties=db_properties)\n",
    "\n",
    "\n",
    "# Stop the Spark session (at the end of the notebook)\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
