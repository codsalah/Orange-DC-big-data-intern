## Overview

This document describes the process for running a Hadoop MapReduce job to perform a word count operation and how to handle the output.
## Steps

- Check installation of Java and Hadoop
![Check installation of Java and Hadoop](imgs/screenshot19.png)

- Running Hadoop
![Running Hadoop](imgs/screenshot3.png)

- Check daemons
![Check daemons](imgs/screenshot4.png)

- Make HDFS directory and double-check it (CLI and UI)
![Make HDFS directory](imgs/screenshot5.png)
![Double-check HDFS directory](imgs/screenshot6.png)

- Specify location of classes that the compiler needs
![Specify location of classes](imgs/screenshot8.png)

- Double-check the output file (CLI and UI)
![Check output file in CLI](imgs/screenshot11.png)
![Check output file in UI](imgs/screenshot10.png)

- Get the output on local machine
![Get output on local machine](imgs/screenshot14.png)

- Open the output file
![Open the output file](imgs/screenshot13.png)

## Notes

- Ensure that you have the appropriate permissions and configurations for HDFS and Git operations.
- Always double-check the paths and commands before execution to avoid mistakes.
