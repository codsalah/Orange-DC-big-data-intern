# Orange-DC-big-data-intern
This repository contains various projects and tasks completed during my internship at Orange Digital Center, where I gained hands-on experience with tools such as **Hadoop**, **Docker**, **Spark**, **Hive**, **Apache NiFi**, **PostgreSQL**, **data warehousing** and More. These projects cover a broad spectrum of data engineering workflows, real-time data processing, and predictive analytics.

## Table of Contents

- [About](#about)
- [Technologies Used](#technologies-used)
- [Projects Overview](#projects-overview)
- [Installation and Setup](#installation-and-setup)
- [Final Words](#final-words)

## About

This repository showcases various tasks and projects focusing on big data processing, real-time analytics, data warehousing, and machine learning for predictive maintenance. The projects utilize a combination of Docker containers, Hadoop ecosystem tools, Apache NiFi, and machine learning models to handle and analyze large datasets efficiently.

## Technologies Used

- **Docker**: Containerization for reproducible environments.
- **Hadoop**: Distributed storage and processing of large datasets.
- **Spark**: Fast, in-memory processing for big data workloads.
- **Hive**: Data warehousing solution built on Hadoop.
- **Apache NiFi**: Automating data flow and integration between systems.
- **PostgreSQL**: Relational database management system for structured data.
- **MongoDB**: NoSQL database for document-based data storage.
- **Machine Learning**: Predictive maintenance using time series data.
- **Python**: Scripting and data analysis.
- **Java**: Used for low-level labs and implementation of foundational components.

## Projects Overview

### 1. **Hadoop Tasks**

- This section includes tasks related to Hadoop ecosystem, focusing on distributed file storage and processing.
- **Key Skills**: HDFS commands, MapReduce, YARN, and HBase integration.
  - Determining_Geographic_Location
  - Java_word_count
  - Twitter_Sentiment_Analysis
  - word_count

### 2. **Hive Lab with Docker**

- A lab-based project using Hive to create data warehouses in a Dockerized environment.
- **Key Skills**: data warehousing concepts, Dockerized deployment.

### 3. **Hive and PostgreSQL Data Warehousing Task**

- In this task, we integrated Hive with PostgreSQL to set up a robust data warehousing solution.
- **Key Skills**: HiveQL, SQL queries, DWH design, data migration between Hive and PostgreSQL.

### 4. **Apache NiFi with MongoDB**

- Automation of data flow between systems using Apache NiFi and MongoDB.
- **Key Skills**: Data ingestion, real-time processing, integration of MongoDB with NiFi.

### 5. **Real-Time IoT Data Processing and Analytics Pipeline**

- Developed a pipeline to handle real-time IoT data using Apache NiFi, Spark, and MongoDB for data storage and analytics.
- **Key Skills**: Data streaming, real-time analytics, IoT data processing.

### 6. **Spark Tasks**

- This section contains Spark-based tasks, focusing on large-scale data processing and in-memory computations.
- **Key Skills**: PySpark, DataFrames, RDD operations.

### 7. **Predictive Maintenance with Time Series Data**

- Built a predictive maintenance model using time-series data to forecast equipment failure.
- **Key Skills**: Machine learning (MLlib), Spark, time series analysis, predictive modeling.

### 8. **Dockerized Hive Lab**

- A Dockerized environment to work with Apache Hive and build a data warehouse system.
- **Key Skills**: Docker, Hive configuration, data partitioning, and indexing.

### 9. **Predictive Maintenance using Spark MLlib and MongoDB**

- A complete pipeline for predictive maintenance using Spark's MLlib for machine learning and MongoDB for storing the results.
- **Key Skills**: Spark MLlib, MongoDB, machine learning models, real-time data processing.

### 10. **Time Series for Predictive Maintenance**

- A comprehensive project focused on analyzing time-series data to predict maintenance needs for sensors and equipment.
- **Key Skills**: Time-series analysis, predictive modeling, Jupyter Notebooks, Docker.


## Installation and Setup

### Prerequisites

Ensure the following are installed locally:

- Docker
- Docker Compose

### Setup

1. Clone the repository:

   ```bash
   git clone https://github.com/codsalah/Orange-DC-big-data-intern.git
   cd internship-projects
   ```

2. Navigate to the respective project folder and run the Docker environment using `docker-compose`.

   Example:

   ```bash
   cd Hive_lab_Docker
   docker-compose up
   ```

### Note

Each project folder contains detailed documentation and setup instructions specific to that project. 
Check the `README.md` or instructions file within each folder for further guidance.


3. Run the respective tasks as outlined in the project instructions.

### Additional Setup Instructions

For detailed instructions on how to install Hadoop, Spark, NiFi, and other tools used in this repository, please refer to the documentation provided in each project folder.


## Final Words
This repository represents the culmination of a challenging and rewarding internship, where I had the opportunity to work on various real-world data engineering and analytics projects using industry-leading tools and technologies.

It was a great learning experience, enhancing my technical skills and broadening my understanding of data processing and infrastructure.

### Special Thanks
I would like to extend my sincere thanks to Eng. [@TawfikYasser](https://github.com/TawfikYasser) , whose guidance and instruction were invaluable throughout this journey. His mentorship played a crucial role in the success of these projects and my overall development during the internship.

### Contact Me
If you have any questions, suggestions, or feedback, feel free to reach out:
- **Email**: [My Email](mailto:salahalgamasy@gmail.com)
- **LinkedIn**: [My LinkedIn](https://www.linkedin.com/in/salah-muhammad-65287b243/)

